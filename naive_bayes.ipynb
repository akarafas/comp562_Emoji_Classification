{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1605487886246",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Squad arriving for Game 2 ğŸš€\n\nDude is like 5â€™8 140 pounds his dick was long and strong(always the little dudes carrying the ğŸ†) ğŸ¤ªğŸ™ƒ\n\nFOLLOWERSğŸ‘‡\n\nI CANT BREATIUHW ğŸ’€ğŸ’€ğŸ’€\n\n2ï¸âƒ£4ï¸âƒ£ hours 'til our schedule drops!\n\n"
    }
   ],
   "source": [
    "tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "number_of_lines = 5\n",
    "for i in range(number_of_lines):\n",
    "    line = tweetFile.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['ğŸ¤', 'ğŸ˜Ÿ', 'ğŸ˜©', 'ğŸ™ƒ', 'ğŸ˜’', 'ğŸ˜¤', 'ğŸ˜«', 'ğŸ‘', 'ğŸ¤”', 'ğŸ˜…', 'ğŸ˜“', 'ğŸ˜­', 'ğŸ¤§', 'ğŸ˜ˆ', 'ğŸ™', 'ğŸ˜ª', 'ğŸ˜´', 'â˜ ï¸', 'ğŸ’€', 'ğŸ’©', 'ğŸ™€', 'ğŸ˜±', 'ğŸ™„', 'ğŸ˜¡', 'ğŸ‘Š', 'ğŸ˜¾', 'ğŸ˜£', 'ğŸ˜”', 'ğŸ˜®', 'ğŸ˜¶', 'ğŸ˜', 'ğŸ¤¢', 'ğŸ˜·', 'ğŸ¤¥', 'ğŸ‘¿', 'ğŸ˜¯', 'ğŸ˜¬', 'ğŸ‘»', 'ğŸ–•', 'â˜¹ï¸', 'ğŸ˜¦', 'ğŸ˜³', 'ğŸ˜¨', 'ğŸ¤’', 'ğŸ¤•', 'ğŸ˜‘', 'ğŸ˜µ', 'ğŸ˜¥', 'ğŸ˜', 'ğŸ˜¿', 'ğŸ˜¢', 'ğŸ˜•', 'ğŸ˜–', 'ğŸ˜°']\n['ğŸ˜‹', 'ğŸ’›', 'ğŸ˜‰', 'âœŒï¸', 'ğŸ’•', 'ğŸ‘', 'ğŸ˜', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜›', 'ğŸ’–', 'ğŸ˜¼', 'ğŸ˜', 'ğŸ˜º', 'ğŸ˜ƒ', 'ğŸ˜¸', 'ğŸ˜„', 'ğŸ™‚', 'ğŸ¤£', 'ğŸ’', 'ğŸ˜Œ', 'ğŸ™Œ', 'ğŸ’œ', 'ğŸ™', 'ğŸ‘Œ', 'ğŸ¤“', 'ğŸ¤‘', 'ğŸ‘„', 'ğŸ˜†', 'ğŸ˜™', 'ğŸ˜˜', 'ğŸ˜š', 'ğŸ˜½', 'ğŸ˜—', 'ğŸ’‹', 'ğŸ˜¹', 'ğŸ˜‚', 'ğŸ˜‡', 'ğŸ¤—', 'ğŸ’—', 'ğŸ’“', 'ğŸ˜»', 'ğŸ˜', 'â¤ï¸', 'ğŸ¤', 'ğŸ˜€', 'ğŸ˜', 'ğŸ’š', 'ğŸ’', 'ğŸ¤¤', 'ğŸ’˜', 'ğŸ¤', 'ğŸ¤ ', 'ğŸ‘', 'ğŸ˜Š', 'ğŸ’™', 'ğŸ–¤', 'ğŸ’¯', 'ğŸ¤ª']\n54\n59\n"
    }
   ],
   "source": [
    "negativeEmojis = ['ğŸ¤', 'ğŸ˜Ÿ', 'ğŸ˜©', 'ğŸ™ƒ', 'ğŸ˜’', 'ğŸ˜¤', 'ğŸ˜«', 'ğŸ‘', 'ğŸ¤”', 'ğŸ˜…', 'ğŸ˜“', 'ğŸ˜­', 'ğŸ¤§', 'ğŸ˜ˆ', 'ğŸ™','ğŸ˜ª', 'ğŸ˜´', 'â˜ ï¸', 'ğŸ’€', 'ğŸ’©', 'ğŸ™€', 'ğŸ˜±', 'ğŸ™„', 'ğŸ˜¡',\n",
    "'ğŸ‘Š', 'ğŸ˜¾', 'ğŸ˜£', 'ğŸ˜”','ğŸ˜®', 'ğŸ˜¶', 'ğŸ˜', 'ğŸ¤¢', 'ğŸ˜·','ğŸ¤¥', 'ğŸ‘¿','ğŸ˜¯', 'ğŸ˜¬', 'ğŸ‘»', 'ğŸ–•', 'â˜¹ï¸', 'ğŸ˜¦', 'ğŸ˜³', 'ğŸ˜¨', 'ğŸ¤’', 'ğŸ¤•', 'ğŸ˜‘', 'ğŸ˜µ', 'ğŸ˜¥', 'ğŸ˜', 'ğŸ˜¿', 'ğŸ˜¢','ğŸ˜•',\n",
    "'ğŸ˜–', 'ğŸ˜°']\n",
    "positiveEmojis = ['ğŸ˜‹', 'ğŸ’›', 'ğŸ˜‰', 'âœŒï¸', 'ğŸ’•', 'ğŸ‘', 'ğŸ˜', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜›', 'ğŸ’–', 'ğŸ˜¼', 'ğŸ˜', 'ğŸ˜º', 'ğŸ˜ƒ','ğŸ˜¸', 'ğŸ˜„', 'ğŸ™‚', 'ğŸ¤£', 'ğŸ’', 'ğŸ˜Œ', 'ğŸ™Œ', 'ğŸ’œ', 'ğŸ™', 'ğŸ‘Œ', 'ğŸ¤“', 'ğŸ¤‘','ğŸ‘„', 'ğŸ˜†', 'ğŸ˜™', 'ğŸ˜˜', 'ğŸ˜š','ğŸ˜½', 'ğŸ˜—', 'ğŸ’‹', 'ğŸ˜¹', 'ğŸ˜‚', 'ğŸ˜‡', 'ğŸ¤—', 'ğŸ’—', 'ğŸ’“', 'ğŸ˜»', 'ğŸ˜', 'â¤ï¸', 'ğŸ¤', 'ğŸ˜€', 'ğŸ˜', 'ğŸ’š', 'ğŸ’', 'ğŸ¤¤','ğŸ’˜', 'ğŸ¤', 'ğŸ¤ ', 'ğŸ‘', 'ğŸ˜Š', 'ğŸ’™', 'ğŸ–¤', 'ğŸ’¯', 'ğŸ¤ª']\n",
    "print(negativeEmojis)\n",
    "print(positiveEmojis)\n",
    "print(len(negativeEmojis))\n",
    "print(len(positiveEmojis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6451\n6451\n6451\n"
    }
   ],
   "source": [
    "# tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "# need to go through and add labels\n",
    "labeledTweetMatrix = []\n",
    "tweets_x = []\n",
    "tweets_y = []\n",
    "tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "# for i in tweetFile:\n",
    "for i in range(10000):\n",
    "    line = tweetFile.readline()\n",
    "    if any(emoji in line for emoji in positiveEmojis):\n",
    "        labeledTweetMatrix.append([line.rstrip(\"\\n\"), 1])\n",
    "        tweets_x.append(line.rstrip(\"\\n\"))\n",
    "        tweets_y.append(1)\n",
    "    elif any(emoji in line for emoji in negativeEmojis):\n",
    "        # labelVector[i] = -1\n",
    "        labeledTweetMatrix.append([line.rstrip(\"\\n\"), -1])\n",
    "        tweets_x.append(line.rstrip(\"\\n\"))\n",
    "        tweets_y.append(-1)\n",
    "\n",
    "    # this will assign all with a +1 emoji 1 and anything else (negative or 0 sentiment) -1\n",
    "print(len(labeledTweetMatrix))\n",
    "print(len(tweets_x))\n",
    "print(len(tweets_y))\n",
    "# print(labeledTweetMatrix[5000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dude is like 5â€™8 140 pounds his dick was long and strong(always the little dudes carrying the ğŸ†) ğŸ¤ªğŸ™ƒ\n1\nI CANT BREATIUHW ğŸ’€ğŸ’€ğŸ’€\n-1\nI am SO scared of birdsğŸ¤§\n-1\nThatâ€™s me ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚\n1\nMy heart is so full rn ğŸ’–ğŸ’–\n1\n"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # print(labeledTweetMatrix[i])\n",
    "    print(tweets_x[i])\n",
    "    print(tweets_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = 'Positive', 'Negative'\n",
    "# go through labeledTweetMatrix and tally positive sentiment, neg just do len(total) - pos\n",
    "count = 0\n",
    "negativeCount = 0\n",
    "for tweet in labeledTweetMatrix:\n",
    "    if tweet[1] == 1:\n",
    "        count += 1\n",
    "    if tweet[1] == -1:\n",
    "        negativeCount += 1\n",
    "sizes = [count, negativeCount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Dana\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "# need to preprocess\n",
    "#pip install tweet-preprocessor needed\n",
    "import nltk                                # Python library for NLP\n",
    "import preprocessor as p\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "def tokenize(tweet):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    return tweet_tokens\n",
    "def removeStopWords(tweet_tokenz):\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    stopwords_english.extend(['rt', 'fav', \"'\"])\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokenz:\n",
    "        if (word not in stopwords_english and word not in string.punctuation):\n",
    "            tweets_clean.append(word)\n",
    "    return tweets_clean\n",
    "def stemmingTweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    tweets_stem = []\n",
    "    for word in tweet:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        tweets_stem.append(stem_word)\n",
    "    return tweets_stem\n",
    "def preprocess(tweet):\n",
    "    tweet.lower()\n",
    "    p.set_options(p.OPT.NUMBER, p.OPT.SMILEY)\n",
    "    tweet = p.clean(tweet)\n",
    "    return stemmingTweet(removeStopWords(tokenize(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dude is like 5â€™8 140 pounds his dick was long and strong(always the little dudes carrying the ğŸ†) ğŸ¤ªğŸ™ƒ\n['dude', 'like', 'â€™', '8', 'pound', 'dick', 'long', 'strong', 'alway', 'littl', 'dude', 'carri', 'ğŸ†', 'ğŸ¤ª', 'ğŸ™ƒ']\n['dude', 'like', 'â€™', '8', 'pound', 'dick', 'long', 'strong', 'alway', 'littl', 'dude', 'carri', 'ğŸ†', 'ğŸ¤ª', 'ğŸ™ƒ']\n['cant', 'breatiuhw', 'ğŸ’€', 'ğŸ’€', 'ğŸ’€']\n['scare', 'bird', 'ğŸ¤§']\n['â€™', 'ğŸ˜‚', 'ğŸ˜‚', 'ğŸ˜‚']\n['heart', 'full', 'rn', 'ğŸ’–', 'ğŸ’–']\n"
    }
   ],
   "source": [
    "# example of preprocessing\n",
    "tweet = tweets_x[0]\n",
    "newtweet = preprocess(tweet)\n",
    "print(tweet)\n",
    "print(newtweet)\n",
    "# preprocess entire matrix\n",
    "for i in range(len(tweets_x)):\n",
    "    processedTweet = preprocess(tweets_x[i])\n",
    "    tweets_x[i] = processedTweet\n",
    "for i in range(5):\n",
    "    print(tweets_x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6451\n4516\n1935\n4516\n1935\n"
    }
   ],
   "source": [
    "# splitting train/test up 70/30\n",
    "div = round(len(tweets_x)*.7)\n",
    "train_x = tweets_x[:div]\n",
    "test_x = tweets_x[div:]\n",
    "train_y = tweets_y[:div]\n",
    "test_y = tweets_y[div:]\n",
    "print(len(tweets_x))\n",
    "print(len(train_x))\n",
    "print(len(test_x))\n",
    "print(len(train_y))\n",
    "print(len(test_x))\n",
    "assert(len(tweets_x) == len(train_x)+len(test_y))\n",
    "assert(len(tweets_x) == len(train_y)+len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will build a dictionary of word frequencies\n",
    "# train_x and train_y is what we will build our word frequency dictionary from\n",
    "# we will need to make our arrays into lists\n",
    "import numpy as np\n",
    "def word_freqs(trainx, trainy):\n",
    "    train_x_list = np.squeeze(trainx).tolist()\n",
    "    train_y_list = np.squeeze(trainy).tolist()\n",
    "    frequencyDict = {}\n",
    "    for y, tweetx in zip(train_y_list, train_x_list):\n",
    "        for word in tweetx:\n",
    "            labeledTweet = (word, y)\n",
    "            # if word is in dict, add 1 to count, otherwise add to dict         \n",
    "            if labeledTweet in frequencyDict:\n",
    "                frequencyDict[labeledTweet]+=1\n",
    "            else:\n",
    "                frequencyDict[labeledTweet]=1\n",
    "    return frequencyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{}\n"
    }
   ],
   "source": [
    "# sample\n",
    "freqs = word_freqs(train_x[15000:15005], train_y[15000:15005])\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6762\n"
    }
   ],
   "source": [
    "# build entire frequency dictionary\n",
    "freqDict = word_freqs(train_x, train_y)\n",
    "print(len(freqDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "29225\n8831\n4594\n2168\n"
    }
   ],
   "source": [
    "#get number of unique words\n",
    "uniqueWords = set([pair[0] for pair in freqDict.keys()])\n",
    "\n",
    "numPositive = 0\n",
    "numNegative = 0\n",
    "uniquePositive = 0\n",
    "uniqueNegative = 0\n",
    "\n",
    "for pair in freqDict.keys():\n",
    "    #if this word is positive\n",
    "    if pair[1] > 0:\n",
    "        numPositive += freqDict[pair]\n",
    "        uniquePositive += 1\n",
    "    #if this word is negative\n",
    "    else:\n",
    "        numNegative += freqDict[pair]\n",
    "        uniqueNegative += 1\n",
    "\n",
    "print(numPositive)\n",
    "print(numNegative)\n",
    "print(uniquePositive)\n",
    "print(uniqueNegative)\n",
    "\n",
    "#get total numbers of neg/positive in training data\n",
    "totalTraining = len(train_y)\n",
    "trainPositive = 0\n",
    "trainNegative = 0\n",
    "for item in train_y:\n",
    "    if item > 0:\n",
    "        trainPositive += 1\n",
    "    else:\n",
    "        trainNegative += 1    \n",
    "\n",
    "probPositive = trainPositive/totalTraining\n",
    "probNegative = 1 - probPositive\n",
    "logPrior = np.log(probPositive) - np.log(probNegative)\n",
    "\n",
    "#iterate through all the unique words and figure out log likelihoods\n",
    "#look at lookup function\n",
    "\n",
    "logWords = {}\n",
    "\n",
    "for word in uniqueWords:\n",
    "    pair = (word, 1)\n",
    "    n = 0\n",
    "    if (pair in freqDict):\n",
    "        n = freqDict[pair]\n",
    "    logPos = (n + 1.)/(numPositive + len(uniqueWords))\n",
    "\n",
    "    pair = (word, -1)\n",
    "    n = 0\n",
    "    if (pair in freqDict):\n",
    "        n = freqDict[pair]\n",
    "    logNeg = (n + 1.)/(numNegative + len(uniqueWords))\n",
    "\n",
    "    logWords[word] = np.log(logPos/logNeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['ğŸ˜‹', 21, 0], ['ğŸ’›', 56, 0], ['ğŸ˜‰', 89, 0], ['âœŒï¸', 0, 0], ['ğŸ’•', 212, 0], ['ğŸ‘', 46, 0], ['ğŸ˜', 58, 0], ['ğŸ˜œ', 31, 0], ['ğŸ˜', 24, 0], ['ğŸ˜›', 20, 0], ['ğŸ’–', 127, 0], ['ğŸ˜¼', 4, 0], ['ğŸ˜', 30, 0], ['ğŸ˜º', 0, 0], ['ğŸ˜ƒ', 14, 0], ['ğŸ˜¸', 5, 0], ['ğŸ˜„', 24, 0], ['ğŸ™‚', 15, 0], ['ğŸ¤£', 198, 0], ['ğŸ’', 67, 0], ['ğŸ˜Œ', 26, 0], ['ğŸ™Œ', 150, 0], ['ğŸ’œ', 104, 0], ['ğŸ™', 242, 0], ['ğŸ‘Œ', 72, 0], ['ğŸ¤“', 3, 0], ['ğŸ¤‘', 11, 0], ['ğŸ‘„', 6, 0], ['ğŸ˜†', 64, 0], ['ğŸ˜™', 8, 0], ['ğŸ˜˜', 96, 0], ['ğŸ˜š', 14, 0], ['ğŸ˜½', 4, 0], ['ğŸ˜—', 9, 0], ['ğŸ’‹', 47, 0], ['ğŸ˜¹', 5, 0], ['ğŸ˜‚', 1679, 0], ['ğŸ˜‡', 22, 0], ['ğŸ¤—', 86, 0], ['ğŸ’—', 112, 0], ['ğŸ’“', 48, 0], ['ğŸ˜»', 28, 0], ['ğŸ˜', 475, 0], ['â¤ï¸', 0, 0], ['ğŸ¤', 2, 0], ['ğŸ˜€', 32, 0], ['ğŸ˜', 54, 0], ['ğŸ’š', 59, 0], ['ğŸ’', 12, 0], ['ğŸ¤¤', 45, 0], ['ğŸ’˜', 36, 0], ['ğŸ¤', 23, 0], ['ğŸ¤ ', 15, 0], ['ğŸ‘', 170, 0], ['ğŸ˜Š', 117, 0], ['ğŸ’™', 90, 0], ['ğŸ–¤', 47, 0], ['ğŸ’¯', 138, 0], ['ğŸ¤ª', 36, 0], ['ğŸ¤', 1, 7], ['ğŸ˜Ÿ', 1, 5], ['ğŸ˜©', 89, 126], ['ğŸ™ƒ', 5, 48], ['ğŸ˜’', 4, 25], ['ğŸ˜¤', 5, 30], ['ğŸ˜«', 12, 18], ['ğŸ‘', 0, 10], ['ğŸ¤”', 12, 95], ['ğŸ˜…', 9, 34], ['ğŸ˜“', 0, 3], ['ğŸ˜­', 292, 426], ['ğŸ¤§', 6, 26], ['ğŸ˜ˆ', 7, 30], ['ğŸ™', 0, 10], ['ğŸ˜ª', 3, 16], ['ğŸ˜´', 3, 20], ['â˜ ï¸', 0, 0], ['ğŸ’€', 52, 71], ['ğŸ’©', 0, 20], ['ğŸ™€', 0, 8], ['ğŸ˜±', 4, 35], ['ğŸ™„', 16, 86], ['ğŸ˜¡', 0, 13], ['ğŸ‘Š', 12, 18], ['ğŸ˜¾', 0, 1], ['ğŸ˜£', 4, 10], ['ğŸ˜”', 8, 39], ['ğŸ˜®', 1, 61], ['ğŸ˜¶', 1, 6], ['ğŸ˜', 1, 16], ['ğŸ¤¢', 3, 7], ['ğŸ˜·', 1, 2], ['ğŸ¤¥', 0, 4], ['ğŸ‘¿', 0, 2], ['ğŸ˜¯', 0, 4], ['ğŸ˜¬', 0, 37], ['ğŸ‘»', 1, 9], ['ğŸ–•', 3, 10], ['â˜¹ï¸', 0, 0], ['ğŸ˜¦', 0, 1], ['ğŸ˜³', 10, 29], ['ğŸ˜¨', 0, 0], ['ğŸ¤’', 0, 6], ['ğŸ¤•', 0, 2], ['ğŸ˜‘', 0, 13], ['ğŸ˜µ', 1, 17], ['ğŸ˜¥', 0, 13], ['ğŸ˜', 7, 17], ['ğŸ˜¿', 0, 1], ['ğŸ˜¢', 7, 35], ['ğŸ˜•', 4, 20], ['ğŸ˜–', 0, 1], ['ğŸ˜°', 0, 9]]\n"
    }
   ],
   "source": [
    "# Analyze how many times our emojis appear in our frequency dictionary\n",
    "keys = positiveEmojis + negativeEmojis\n",
    "data = []\n",
    "for emoji in keys:\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    if (emoji, 1) in freqDict:\n",
    "        positive = freqDict[(emoji, 1)]\n",
    "    if (emoji, -1) in freqDict:\n",
    "        negative = freqDict[(emoji, -1)]\n",
    "    data.append([emoji, positive, negative])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output for positive example:  11.844736175187961\nOutput for negative example:  -1.8456467328620865\n"
    }
   ],
   "source": [
    "def naiveBayes(tweet):\n",
    "    pTweet = preprocess(tweet)\n",
    "    prob = 0\n",
    "\n",
    "    for word in pTweet:\n",
    "        if word in logWords:\n",
    "            prob += logWords[word]\n",
    "    prob += logPrior\n",
    "\n",
    "    return prob\n",
    "\n",
    "newTweet = 'I am happy and glad and smiling and I love my life â¤ï¸'\n",
    "prob = naiveBayes(newTweet)\n",
    "print('Output for positive example: ', prob)\n",
    "newTweet1 = 'I am sad and I hate my life and everything sucks ğŸ¤¢'\n",
    "prob = naiveBayes(newTweet1)\n",
    "print('Output for negative example: ', prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-e1cd42ce96ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-162-e1cd42ce96ce>\u001b[0m in \u001b[0;36mtestAccuracy\u001b[1;34m(test_x, test_y)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m#print(tweet1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaiveBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-161-cc7d408885bd>\u001b[0m in \u001b[0;36mnaiveBayes\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mnaiveBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mpTweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpTweet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-153-19a2f23d5b07>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtweets_stem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUMBER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSMILEY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "def testAccuracy(test_x, test_y):\n",
    "    for tweet in test_x:\n",
    "        #print(tweet1)\n",
    "        prob = naiveBayes(tweet)\n",
    "        if (prob > 0):\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "    error = np.mean(np.absolute(test_y - predictions))\n",
    "    return 1-error\n",
    "\n",
    "print('Accuracy: ', testAccuracy(test_x, test_y))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}