{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1605487886246",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Squad arriving for Game 2 🚀\n\nDude is like 5’8 140 pounds his dick was long and strong(always the little dudes carrying the 🍆) 🤪🙃\n\nFOLLOWERS👇\n\nI CANT BREATIUHW 💀💀💀\n\n2️⃣4️⃣ hours 'til our schedule drops!\n\n"
    }
   ],
   "source": [
    "tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "number_of_lines = 5\n",
    "for i in range(number_of_lines):\n",
    "    line = tweetFile.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['🤐', '😟', '😩', '🙃', '😒', '😤', '😫', '👎', '🤔', '😅', '😓', '😭', '🤧', '😈', '🙁', '😪', '😴', '☠️', '💀', '💩', '🙀', '😱', '🙄', '😡', '👊', '😾', '😣', '😔', '😮', '😶', '😐', '🤢', '😷', '🤥', '👿', '😯', '😬', '👻', '🖕', '☹️', '😦', '😳', '😨', '🤒', '🤕', '😑', '😵', '😥', '😞', '😿', '😢', '😕', '😖', '😰']\n['😋', '💛', '😉', '✌️', '💕', '👍', '😎', '😜', '😝', '😛', '💖', '😼', '😏', '😺', '😃', '😸', '😄', '🙂', '🤣', '💞', '😌', '🙌', '💜', '🙏', '👌', '🤓', '🤑', '👄', '😆', '😙', '😘', '😚', '😽', '😗', '💋', '😹', '😂', '😇', '🤗', '💗', '💓', '😻', '😍', '❤️', '🤝', '😀', '😁', '💚', '💝', '🤤', '💘', '🤞', '🤠', '👏', '😊', '💙', '🖤', '💯', '🤪']\n54\n59\n"
    }
   ],
   "source": [
    "negativeEmojis = ['🤐', '😟', '😩', '🙃', '😒', '😤', '😫', '👎', '🤔', '😅', '😓', '😭', '🤧', '😈', '🙁','😪', '😴', '☠️', '💀', '💩', '🙀', '😱', '🙄', '😡',\n",
    "'👊', '😾', '😣', '😔','😮', '😶', '😐', '🤢', '😷','🤥', '👿','😯', '😬', '👻', '🖕', '☹️', '😦', '😳', '😨', '🤒', '🤕', '😑', '😵', '😥', '😞', '😿', '😢','😕',\n",
    "'😖', '😰']\n",
    "positiveEmojis = ['😋', '💛', '😉', '✌️', '💕', '👍', '😎', '😜', '😝', '😛', '💖', '😼', '😏', '😺', '😃','😸', '😄', '🙂', '🤣', '💞', '😌', '🙌', '💜', '🙏', '👌', '🤓', '🤑','👄', '😆', '😙', '😘', '😚','😽', '😗', '💋', '😹', '😂', '😇', '🤗', '💗', '💓', '😻', '😍', '❤️', '🤝', '😀', '😁', '💚', '💝', '🤤','💘', '🤞', '🤠', '👏', '😊', '💙', '🖤', '💯', '🤪']\n",
    "print(negativeEmojis)\n",
    "print(positiveEmojis)\n",
    "print(len(negativeEmojis))\n",
    "print(len(positiveEmojis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6451\n6451\n6451\n"
    }
   ],
   "source": [
    "# tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "# need to go through and add labels\n",
    "labeledTweetMatrix = []\n",
    "tweets_x = []\n",
    "tweets_y = []\n",
    "tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "# for i in tweetFile:\n",
    "for i in range(10000):\n",
    "    line = tweetFile.readline()\n",
    "    if any(emoji in line for emoji in positiveEmojis):\n",
    "        labeledTweetMatrix.append([line.rstrip(\"\\n\"), 1])\n",
    "        tweets_x.append(line.rstrip(\"\\n\"))\n",
    "        tweets_y.append(1)\n",
    "    elif any(emoji in line for emoji in negativeEmojis):\n",
    "        # labelVector[i] = -1\n",
    "        labeledTweetMatrix.append([line.rstrip(\"\\n\"), -1])\n",
    "        tweets_x.append(line.rstrip(\"\\n\"))\n",
    "        tweets_y.append(-1)\n",
    "\n",
    "    # this will assign all with a +1 emoji 1 and anything else (negative or 0 sentiment) -1\n",
    "print(len(labeledTweetMatrix))\n",
    "print(len(tweets_x))\n",
    "print(len(tweets_y))\n",
    "# print(labeledTweetMatrix[5000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dude is like 5’8 140 pounds his dick was long and strong(always the little dudes carrying the 🍆) 🤪🙃\n1\nI CANT BREATIUHW 💀💀💀\n-1\nI am SO scared of birds🤧\n-1\nThat’s me 😂😂😂😂😂😂\n1\nMy heart is so full rn 💖💖\n1\n"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # print(labeledTweetMatrix[i])\n",
    "    print(tweets_x[i])\n",
    "    print(tweets_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = 'Positive', 'Negative'\n",
    "# go through labeledTweetMatrix and tally positive sentiment, neg just do len(total) - pos\n",
    "count = 0\n",
    "negativeCount = 0\n",
    "for tweet in labeledTweetMatrix:\n",
    "    if tweet[1] == 1:\n",
    "        count += 1\n",
    "    if tweet[1] == -1:\n",
    "        negativeCount += 1\n",
    "sizes = [count, negativeCount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Dana\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "# need to preprocess\n",
    "#pip install tweet-preprocessor needed\n",
    "import nltk                                # Python library for NLP\n",
    "import preprocessor as p\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "def tokenize(tweet):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    return tweet_tokens\n",
    "def removeStopWords(tweet_tokenz):\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    stopwords_english.extend(['rt', 'fav', \"'\"])\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokenz:\n",
    "        if (word not in stopwords_english and word not in string.punctuation):\n",
    "            tweets_clean.append(word)\n",
    "    return tweets_clean\n",
    "def stemmingTweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    tweets_stem = []\n",
    "    for word in tweet:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        tweets_stem.append(stem_word)\n",
    "    return tweets_stem\n",
    "def preprocess(tweet):\n",
    "    tweet.lower()\n",
    "    p.set_options(p.OPT.NUMBER, p.OPT.SMILEY)\n",
    "    tweet = p.clean(tweet)\n",
    "    return stemmingTweet(removeStopWords(tokenize(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dude is like 5’8 140 pounds his dick was long and strong(always the little dudes carrying the 🍆) 🤪🙃\n['dude', 'like', '’', '8', 'pound', 'dick', 'long', 'strong', 'alway', 'littl', 'dude', 'carri', '🍆', '🤪', '🙃']\n['dude', 'like', '’', '8', 'pound', 'dick', 'long', 'strong', 'alway', 'littl', 'dude', 'carri', '🍆', '🤪', '🙃']\n['cant', 'breatiuhw', '💀', '💀', '💀']\n['scare', 'bird', '🤧']\n['’', '😂', '😂', '😂']\n['heart', 'full', 'rn', '💖', '💖']\n"
    }
   ],
   "source": [
    "# example of preprocessing\n",
    "tweet = tweets_x[0]\n",
    "newtweet = preprocess(tweet)\n",
    "print(tweet)\n",
    "print(newtweet)\n",
    "# preprocess entire matrix\n",
    "for i in range(len(tweets_x)):\n",
    "    processedTweet = preprocess(tweets_x[i])\n",
    "    tweets_x[i] = processedTweet\n",
    "for i in range(5):\n",
    "    print(tweets_x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6451\n4516\n1935\n4516\n1935\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# splitting train/test up 70/30\n",
    "div = round(len(tweets_x)*.7)\n",
    "train_x = tweets_x[:div]\n",
    "test_x = tweets_x[div:]\n",
    "train_y = tweets_y[:div]\n",
    "test_y = tweets_y[div:]\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.array(test_y)\n",
    "print(len(tweets_x))\n",
    "print(len(train_x))\n",
    "print(len(test_x))\n",
    "print(len(train_y))\n",
    "print(len(test_x))\n",
    "assert(len(tweets_x) == len(train_x)+len(test_y))\n",
    "assert(len(tweets_x) == len(train_y)+len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will build a dictionary of word frequencies\n",
    "# train_x and train_y is what we will build our word frequency dictionary from\n",
    "# we will need to make our arrays into lists\n",
    "#import numpy as np\n",
    "def word_freqs(trainx, trainy):\n",
    "    train_x_list = np.squeeze(trainx).tolist()\n",
    "    train_y_list = np.squeeze(trainy).tolist()\n",
    "    frequencyDict = {}\n",
    "    for y, tweetx in zip(train_y_list, train_x_list):\n",
    "        for word in tweetx:\n",
    "            labeledTweet = (word, y)\n",
    "            # if word is in dict, add 1 to count, otherwise add to dict         \n",
    "            if labeledTweet in frequencyDict:\n",
    "                frequencyDict[labeledTweet]+=1\n",
    "            else:\n",
    "                frequencyDict[labeledTweet]=1\n",
    "    return frequencyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{}\n"
    }
   ],
   "source": [
    "# sample\n",
    "freqs = word_freqs(train_x[15000:15005], train_y[15000:15005])\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6762\n"
    }
   ],
   "source": [
    "# build entire frequency dictionary\n",
    "freqDict = word_freqs(train_x, train_y)\n",
    "print(len(freqDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "29225\n8831\n4594\n2168\n"
    }
   ],
   "source": [
    "#get number of unique words\n",
    "uniqueWords = set([pair[0] for pair in freqDict.keys()])\n",
    "\n",
    "numPositive = 0\n",
    "numNegative = 0\n",
    "uniquePositive = 0\n",
    "uniqueNegative = 0\n",
    "\n",
    "for pair in freqDict.keys():\n",
    "    #if this word is positive\n",
    "    if pair[1] > 0:\n",
    "        numPositive += freqDict[pair]\n",
    "        uniquePositive += 1\n",
    "    #if this word is negative\n",
    "    else:\n",
    "        numNegative += freqDict[pair]\n",
    "        uniqueNegative += 1\n",
    "\n",
    "print(numPositive)\n",
    "print(numNegative)\n",
    "print(uniquePositive)\n",
    "print(uniqueNegative)\n",
    "\n",
    "#get total numbers of neg/positive in training data\n",
    "totalTraining = len(train_y)\n",
    "trainPositive = 0\n",
    "trainNegative = 0\n",
    "for item in train_y:\n",
    "    if item > 0:\n",
    "        trainPositive += 1\n",
    "    else:\n",
    "        trainNegative += 1    \n",
    "\n",
    "probPositive = trainPositive/totalTraining\n",
    "probNegative = 1 - probPositive\n",
    "logPrior = np.log(probPositive) - np.log(probNegative)\n",
    "\n",
    "#iterate through all the unique words and figure out log likelihoods\n",
    "#look at lookup function\n",
    "\n",
    "logWords = {}\n",
    "\n",
    "for word in uniqueWords:\n",
    "    pair = (word, 1)\n",
    "    n = 0\n",
    "    if (pair in freqDict):\n",
    "        n = freqDict[pair]\n",
    "    logPos = (n + 1.)/(numPositive + len(uniqueWords))\n",
    "\n",
    "    pair = (word, -1)\n",
    "    n = 0\n",
    "    if (pair in freqDict):\n",
    "        n = freqDict[pair]\n",
    "    logNeg = (n + 1.)/(numNegative + len(uniqueWords))\n",
    "\n",
    "    logWords[word] = np.log(logPos/logNeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['😋', 21, 0], ['💛', 56, 0], ['😉', 89, 0], ['✌️', 0, 0], ['💕', 212, 0], ['👍', 46, 0], ['😎', 58, 0], ['😜', 31, 0], ['😝', 24, 0], ['😛', 20, 0], ['💖', 127, 0], ['😼', 4, 0], ['😏', 30, 0], ['😺', 0, 0], ['😃', 14, 0], ['😸', 5, 0], ['😄', 24, 0], ['🙂', 15, 0], ['🤣', 198, 0], ['💞', 67, 0], ['😌', 26, 0], ['🙌', 150, 0], ['💜', 104, 0], ['🙏', 242, 0], ['👌', 72, 0], ['🤓', 3, 0], ['🤑', 11, 0], ['👄', 6, 0], ['😆', 64, 0], ['😙', 8, 0], ['😘', 96, 0], ['😚', 14, 0], ['😽', 4, 0], ['😗', 9, 0], ['💋', 47, 0], ['😹', 5, 0], ['😂', 1679, 0], ['😇', 22, 0], ['🤗', 86, 0], ['💗', 112, 0], ['💓', 48, 0], ['😻', 28, 0], ['😍', 475, 0], ['❤️', 0, 0], ['🤝', 2, 0], ['😀', 32, 0], ['😁', 54, 0], ['💚', 59, 0], ['💝', 12, 0], ['🤤', 45, 0], ['💘', 36, 0], ['🤞', 23, 0], ['🤠', 15, 0], ['👏', 170, 0], ['😊', 117, 0], ['💙', 90, 0], ['🖤', 47, 0], ['💯', 138, 0], ['🤪', 36, 0], ['🤐', 1, 7], ['😟', 1, 5], ['😩', 89, 126], ['🙃', 5, 48], ['😒', 4, 25], ['😤', 5, 30], ['😫', 12, 18], ['👎', 0, 10], ['🤔', 12, 95], ['😅', 9, 34], ['😓', 0, 3], ['😭', 292, 426], ['🤧', 6, 26], ['😈', 7, 30], ['🙁', 0, 10], ['😪', 3, 16], ['😴', 3, 20], ['☠️', 0, 0], ['💀', 52, 71], ['💩', 0, 20], ['🙀', 0, 8], ['😱', 4, 35], ['🙄', 16, 86], ['😡', 0, 13], ['👊', 12, 18], ['😾', 0, 1], ['😣', 4, 10], ['😔', 8, 39], ['😮', 1, 61], ['😶', 1, 6], ['😐', 1, 16], ['🤢', 3, 7], ['😷', 1, 2], ['🤥', 0, 4], ['👿', 0, 2], ['😯', 0, 4], ['😬', 0, 37], ['👻', 1, 9], ['🖕', 3, 10], ['☹️', 0, 0], ['😦', 0, 1], ['😳', 10, 29], ['😨', 0, 0], ['🤒', 0, 6], ['🤕', 0, 2], ['😑', 0, 13], ['😵', 1, 17], ['😥', 0, 13], ['😞', 7, 17], ['😿', 0, 1], ['😢', 7, 35], ['😕', 4, 20], ['😖', 0, 1], ['😰', 0, 9]]\n"
    }
   ],
   "source": [
    "# Analyze how many times our emojis appear in our frequency dictionary\n",
    "keys = positiveEmojis + negativeEmojis\n",
    "data = []\n",
    "for emoji in keys:\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    if (emoji, 1) in freqDict:\n",
    "        positive = freqDict[(emoji, 1)]\n",
    "    if (emoji, -1) in freqDict:\n",
    "        negative = freqDict[(emoji, -1)]\n",
    "    data.append([emoji, positive, negative])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output for positive example: 22.081497756192675\nOutput for negative example:  12.032963787976106\n"
    }
   ],
   "source": [
    "def naiveBayes(tweet):\n",
    "    prob = 0\n",
    "    for word in tweet:\n",
    "        if word in logWords:\n",
    "            prob += logWords[word]\n",
    "    prob += logPrior\n",
    "\n",
    "    return prob\n",
    "\n",
    "newTweet = 'I am happy and glad and smiling and I love my life ❤️'\n",
    "prob = naiveBayes(newTweet)\n",
    "print('Output for positive example: ', prob)\n",
    "newTweet1 = 'I am sad and I hate my life and everything sucks 🤢'\n",
    "prob = naiveBayes(newTweet1)\n",
    "print('Output for negative example: ', prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.8801033591731267\n"
    }
   ],
   "source": [
    "predictions = []\n",
    "def testAccuracy(test_x, test_y):\n",
    "    for tweet in test_x:\n",
    "        #print(tweet1)\n",
    "        prob = naiveBayes(tweet)\n",
    "        if (prob > 0):\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "    error = np.mean(np.absolute(test_y - predictions))\n",
    "    return 1-error\n",
    "\n",
    "print('Accuracy: ', testAccuracy(test_x, test_y))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}