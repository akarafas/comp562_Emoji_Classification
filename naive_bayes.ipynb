{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1605377127842",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Squad arriving for Game 2 🚀\n\nDude is like 5’8 140 pounds his dick was long and strong(always the little dudes carrying the 🍆) 🤪🙃\n\nFOLLOWERS👇\n\nI CANT BREATIUHW 💀💀💀\n\n2️⃣4️⃣ hours 'til our schedule drops!\n\n"
    }
   ],
   "source": [
    "tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "number_of_lines = 5\n",
    "for i in range(number_of_lines):\n",
    "    line = tweetFile.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['🤐', '😟', '😩', '🙃', '😒', '😤', '😫', '👎', '🤔', '😅', '😓', '😭', '🤧', '😈', '🙁', '😪', '😴', '☠️', '💀', '💩', '🙀', '😱', '🙄', '😡', '👊', '😾', '😣', '😔', '😮', '😶', '😐', '🤢', '😷', '🤥', '👿', '😯', '😬', '👻', '🖕', '☹️', '😦', '😳', '😨', '🤒', '🤕', '😑', '😵', '😥', '😞', '😿', '😢', '😕', '😖', '😰']\n['😋', '💛', '😉', '✌️', '💕', '👍', '😎', '😜', '😝', '😛', '💖', '😼', '😏', '😺', '😃', '😸', '😄', '🙂', '🤣', '💞', '😌', '🙌', '💜', '🙏', '👌', '🤓', '🤑', '👄', '😆', '😙', '😘', '😚', '😽', '😗', '💋', '😹', '😂', '😇', '🤗', '💗', '💓', '😻', '😍', '❤️', '🤝', '😀', '😁', '💚', '💝', '🤤', '💘', '🤞', '🤠', '👏', '😊', '💙', '🖤', '💯', '🤪']\n54\n59\n"
    }
   ],
   "source": [
    "negativeEmojis = ['🤐', '😟', '😩', '🙃', '😒', '😤', '😫', '👎', '🤔', '😅', '😓', '😭', '🤧', '😈', '🙁','😪', '😴', '☠️', '💀', '💩', '🙀', '😱', '🙄', '😡',\n",
    "'👊', '😾', '😣', '😔','😮', '😶', '😐', '🤢', '😷','🤥', '👿','😯', '😬', '👻', '🖕', '☹️', '😦', '😳', '😨', '🤒', '🤕', '😑', '😵', '😥', '😞', '😿', '😢','😕',\n",
    "'😖', '😰']\n",
    "positiveEmojis = ['😋', '💛', '😉', '✌️', '💕', '👍', '😎', '😜', '😝', '😛', '💖', '😼', '😏', '😺', '😃','😸', '😄', '🙂', '🤣', '💞', '😌', '🙌', '💜', '🙏', '👌', '🤓', '🤑','👄', '😆', '😙', '😘', '😚','😽', '😗', '💋', '😹', '😂', '😇', '🤗', '💗', '💓', '😻', '😍', '❤️', '🤝', '😀', '😁', '💚', '💝', '🤤','💘', '🤞', '🤠', '👏', '😊', '💙', '🖤', '💯', '🤪']\n",
    "print(negativeEmojis)\n",
    "print(positiveEmojis)\n",
    "print(len(negativeEmojis))\n",
    "print(len(positiveEmojis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "31330\n31330\n31330\n"
    }
   ],
   "source": [
    "# tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "# need to go through and add labels\n",
    "labeledTweetMatrix = []\n",
    "tweets_x = []\n",
    "tweets_y = []\n",
    "tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "# for i in tweetFile:\n",
    "for i in range(50000):\n",
    "    line = tweetFile.readline()\n",
    "    if any(emoji in line for emoji in positiveEmojis):\n",
    "        labeledTweetMatrix.append([line.rstrip(\"\\n\"), 1])\n",
    "        tweets_x.append(line.rstrip(\"\\n\"))\n",
    "        tweets_y.append(1)\n",
    "    elif any(emoji in line for emoji in negativeEmojis):\n",
    "        # labelVector[i] = -1\n",
    "        labeledTweetMatrix.append([line.rstrip(\"\\n\"), -1])\n",
    "        tweets_x.append(line.rstrip(\"\\n\"))\n",
    "        tweets_y.append(-1)\n",
    "\n",
    "    # this will assign all with a +1 emoji 1 and anything else (negative or 0 sentiment) -1\n",
    "print(len(labeledTweetMatrix))\n",
    "print(len(tweets_x))\n",
    "print(len(tweets_y))\n",
    "# print(labeledTweetMatrix[5000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dude is like 5’8 140 pounds his dick was long and strong(always the little dudes carrying the 🍆) 🤪🙃\n1\nI CANT BREATIUHW 💀💀💀\n-1\nI am SO scared of birds🤧\n-1\nThat’s me 😂😂😂😂😂😂\n1\nMy heart is so full rn 💖💖\n1\n"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # print(labeledTweetMatrix[i])\n",
    "    print(tweets_x[i])\n",
    "    print(tweets_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'Positive', 'Negative'\n",
    "# go through labeledTweetMatrix and tally positive sentiment, neg just do len(total) - pos\n",
    "count = 0\n",
    "negativeCount = 0\n",
    "for tweet in labeledTweetMatrix:\n",
    "    if tweet[1] == 1:\n",
    "        count += 1\n",
    "    if tweet[1] == -1:\n",
    "        negativeCount += 1\n",
    "sizes = [count, negativeCount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Dana\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "# need to preprocess\n",
    "#pip install tweet-preprocessor needed\n",
    "import nltk                                # Python library for NLP\n",
    "import preprocessor as p\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "def tokenize(tweet):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    return tweet_tokens\n",
    "def removeStopWords(tweet_tokenz):\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    stopwords_english.extend(['rt', 'fav', \"'\"])\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokenz:\n",
    "        if (word not in stopwords_english and word not in string.punctuation):\n",
    "            tweets_clean.append(word)\n",
    "    return tweets_clean\n",
    "def stemmingTweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    tweets_stem = []\n",
    "    for word in tweet:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        tweets_stem.append(stem_word)\n",
    "    return tweets_stem\n",
    "def preprocess(tweet):\n",
    "    tweet.lower()\n",
    "    p.set_options(p.OPT.NUMBER, p.OPT.SMILEY)\n",
    "    tweet = p.clean(tweet)\n",
    "    return stemmingTweet(removeStopWords(tokenize(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dude is like 5’8 140 pounds his dick was long and strong(always the little dudes carrying the 🍆) 🤪🙃\n['dude', 'like', '’', '8', 'pound', 'dick', 'long', 'strong', 'alway', 'littl', 'dude', 'carri', '🍆', '🤪', '🙃']\n['dude', 'like', '’', '8', 'pound', 'dick', 'long', 'strong', 'alway', 'littl', 'dude', 'carri', '🍆', '🤪', '🙃']\n['cant', 'breatiuhw', '💀', '💀', '💀']\n['scare', 'bird', '🤧']\n['’', '😂', '😂', '😂']\n['heart', 'full', 'rn', '💖', '💖']\n"
    }
   ],
   "source": [
    "# example of preprocessing\n",
    "tweet = tweets_x[0]\n",
    "newtweet = preprocess(tweet)\n",
    "print(tweet)\n",
    "print(newtweet)\n",
    "# preprocess entire matrix\n",
    "for i in range(len(tweets_x)):\n",
    "    processedTweet = preprocess(tweets_x[i])\n",
    "    tweets_x[i] = processedTweet\n",
    "for i in range(5):\n",
    "    print(tweets_x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "31330\n21931\n9399\n21931\n9399\n"
    }
   ],
   "source": [
    "# splitting train/test up 70/30\n",
    "div = round(len(tweets_x)*.7)\n",
    "train_x = tweets_x[:div]\n",
    "test_x = tweets_x[div:]\n",
    "train_y = tweets_y[:div]\n",
    "test_y = tweets_y[div:]\n",
    "print(len(tweets_x))\n",
    "print(len(train_x))\n",
    "print(len(test_x))\n",
    "print(len(train_y))\n",
    "print(len(test_x))\n",
    "assert(len(tweets_x) == len(train_x)+len(test_y))\n",
    "assert(len(tweets_x) == len(train_y)+len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will build a dictionary of word frequencies\n",
    "# train_x and train_y is what we will build our word frequency dictionary from\n",
    "# we will need to make our arrays into lists\n",
    "import numpy as np\n",
    "def word_freqs(trainx, trainy):\n",
    "    train_x_list = np.squeeze(trainx).tolist()\n",
    "    train_y_list = np.squeeze(trainy).tolist()\n",
    "    frequencyDict = {}\n",
    "    for y, tweetx in zip(train_y_list, train_x_list):\n",
    "        for word in tweetx:\n",
    "            labeledTweet = (word, y)\n",
    "            # if word is in dict, add 1 to count, otherwise add to dict         \n",
    "            if labeledTweet in frequencyDict:\n",
    "                frequencyDict[labeledTweet]+=1\n",
    "            else:\n",
    "                frequencyDict[labeledTweet]=1\n",
    "    return frequencyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{('world', -1): 1, ('map', -1): 1, ('sound', -1): 1, ('like', -1): 1, ('piano', -1): 1, ('😮', -1): 1, ('nigga', 1): 2, ('said', 1): 2, ('insur', 1): 2, ('frog', 1): 2, ('vs', 1): 2, ('🐸', 1): 6, ('😂', 1): 7, ('final', 1): 1, ('found', 1): 1, ('video', 1): 1, ('😭', 1): 3, ('yerrr', 1): 1, ('post', 1): 1, ('panti', 1): 1, ('pic', 1): 1, ('get', 1): 1, ('readi', 1): 1, ('guy', 1): 1, ('😍', 1): 1}\n"
    }
   ],
   "source": [
    "# sample\n",
    "freqs = word_freqs(train_x[15000:15005], train_y[15000:15005])\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "17426\n"
    }
   ],
   "source": [
    "# build entire frequency dictionary\n",
    "freqDict = word_freqs(train_x, train_y)\n",
    "print(len(freqDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "141406\n42359\n11742\n5684\n105\n1\n1\n1\n5\n"
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "('💉', 1)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-fe2d5d3a7366>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#iterate through all the unique words and figure out log likelihoods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muniqueWords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreqDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: ('💉', 1)"
     ]
    }
   ],
   "source": [
    "#get number of unique words\n",
    "uniqueWords = set([pair[0] for pair in freqDict.keys()])\n",
    "\n",
    "numPositive = 0\n",
    "numNegative = 0\n",
    "uniquePositive = 0\n",
    "uniqueNegative = 0\n",
    "\n",
    "for pair in freqDict.keys():\n",
    "    #if this word is positive\n",
    "    if pair[1] > 0:\n",
    "        numPositive += freqDict[pair]\n",
    "        uniquePositive += 1\n",
    "    #if this word is negative\n",
    "    else:\n",
    "        numNegative += freqDict[pair]\n",
    "        uniqueNegative += 1\n",
    "\n",
    "print(numPositive)\n",
    "print(numNegative)\n",
    "print(uniquePositive)\n",
    "print(uniqueNegative)\n",
    "\n",
    "#get total numbers of neg/positive in training data\n",
    "totalTraining = len(train_y)\n",
    "trainPositive = 0\n",
    "trainNegative = 0\n",
    "for item in train_y:\n",
    "    if item > 0:\n",
    "        trainPositive += 1\n",
    "    else:\n",
    "        trainNegative += 1    \n",
    "\n",
    "probPositive = trainPositive/totalTraining\n",
    "probNegative = 1 - probPositive\n",
    "logPrior = np.log(probPositive) - np.log(probNegative)\n",
    "\n",
    "#iterate through all the unique words and figure out log likelihoods\n",
    "#look at lookup function\n",
    "for word in uniqueWords:\n",
    "    print(freqDict[(word , 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['😋', 138, 0], ['💛', 395, 0], ['😉', 345, 0], ['✌️', 0, 0], ['💕', 1020, 0], ['👍', 336, 0], ['😎', 323, 0], ['😜', 139, 0], ['😝', 86, 0], ['😛', 74, 0], ['💖', 585, 0], ['😼', 12, 0], ['😏', 185, 0], ['😺', 4, 0], ['😃', 96, 0], ['😸', 14, 0], ['😄', 133, 0], ['🙂', 112, 0], ['🤣', 896, 0], ['💞', 351, 0], ['😌', 170, 0], ['🙌', 672, 0], ['💜', 563, 0], ['🙏', 1036, 0], ['👌', 403, 0], ['🤓', 20, 0], ['🤑', 42, 0], ['👄', 28, 0], ['😆', 174, 0], ['😙', 26, 0], ['😘', 526, 0], ['😚', 70, 0], ['😽', 11, 0], ['😗', 23, 0], ['💋', 269, 0], ['😹', 57, 0], ['😂', 8197, 0], ['😇', 114, 0], ['🤗', 288, 0], ['💗', 437, 0], ['💓', 328, 0], ['😻', 94, 0], ['😍', 2301, 0], ['❤️', 0, 0], ['🤝', 14, 0], ['😀', 154, 0], ['😁', 327, 0], ['💚', 332, 0], ['💝', 93, 0], ['🤤', 179, 0], ['💘', 227, 0], ['🤞', 113, 0], ['🤠', 40, 0], ['👏', 940, 0], ['😊', 541, 0], ['💙', 511, 0], ['🖤', 204, 0], ['💯', 634, 0], ['🤪', 152, 0], ['🤐', 3, 18], ['😟', 2, 13], ['😩', 289, 526], ['🙃', 26, 254], ['😒', 13, 136], ['😤', 29, 163], ['😫', 44, 153], ['👎', 11, 23], ['🤔', 68, 486], ['😅', 48, 177], ['😓', 8, 29], ['😭', 1470, 2176], ['🤧', 20, 108], ['😈', 55, 179], ['🙁', 3, 37], ['😪', 11, 80], ['😴', 16, 120], ['☠️', 0, 0], ['💀', 195, 390], ['💩', 8, 50], ['🙀', 0, 14], ['😱', 51, 171], ['🙄', 58, 383], ['😡', 6, 92], ['👊', 46, 81], ['😾', 0, 14], ['😣', 14, 64], ['😔', 25, 168], ['😮', 6, 276], ['😶', 3, 26], ['😐', 6, 67], ['🤢', 6, 46], ['😷', 1, 24], ['🤥', 1, 37], ['👿', 1, 7], ['😯', 3, 33], ['😬', 16, 135], ['👻', 8, 24], ['🖕', 14, 31], ['☹️', 0, 0], ['😦', 0, 10], ['😳', 42, 189], ['😨', 0, 32], ['🤒', 0, 12], ['🤕', 1, 16], ['😑', 3, 57], ['😵', 8, 45], ['😥', 3, 54], ['😞', 17, 67], ['😿', 0, 7], ['😢', 42, 185], ['😕', 6, 64], ['😖', 3, 16], ['😰', 1, 33]]\n"
    }
   ],
   "source": [
    "# Analyze how many times our emojis appear in our frequency dictionary\n",
    "keys = positiveEmojis + negativeEmojis\n",
    "data = []\n",
    "for emoji in keys:\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    if (emoji, 1) in freqDict:\n",
    "        positive = freqDict[(emoji, 1)]\n",
    "    if (emoji, -1) in freqDict:\n",
    "        negative = freqDict[(emoji, -1)]\n",
    "    data.append([emoji, positive, negative])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}