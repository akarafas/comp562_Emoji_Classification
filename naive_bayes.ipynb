{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1605487886246",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Squad arriving for Game 2 ðŸš€\n\nDude is like 5â€™8 140 pounds his dick was long and strong(always the little dudes carrying the ðŸ†) ðŸ¤ªðŸ™ƒ\n\nFOLLOWERSðŸ‘‡\n\nI CANT BREATIUHW ðŸ’€ðŸ’€ðŸ’€\n\n2ï¸âƒ£4ï¸âƒ£ hours 'til our schedule drops!\n\n"
    }
   ],
   "source": [
    "tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "number_of_lines = 5\n",
    "for i in range(number_of_lines):\n",
    "    line = tweetFile.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['ðŸ¤', 'ðŸ˜Ÿ', 'ðŸ˜©', 'ðŸ™ƒ', 'ðŸ˜’', 'ðŸ˜¤', 'ðŸ˜«', 'ðŸ‘Ž', 'ðŸ¤”', 'ðŸ˜…', 'ðŸ˜“', 'ðŸ˜­', 'ðŸ¤§', 'ðŸ˜ˆ', 'ðŸ™', 'ðŸ˜ª', 'ðŸ˜´', 'â˜ ï¸', 'ðŸ’€', 'ðŸ’©', 'ðŸ™€', 'ðŸ˜±', 'ðŸ™„', 'ðŸ˜¡', 'ðŸ‘Š', 'ðŸ˜¾', 'ðŸ˜£', 'ðŸ˜”', 'ðŸ˜®', 'ðŸ˜¶', 'ðŸ˜', 'ðŸ¤¢', 'ðŸ˜·', 'ðŸ¤¥', 'ðŸ‘¿', 'ðŸ˜¯', 'ðŸ˜¬', 'ðŸ‘»', 'ðŸ–•', 'â˜¹ï¸', 'ðŸ˜¦', 'ðŸ˜³', 'ðŸ˜¨', 'ðŸ¤’', 'ðŸ¤•', 'ðŸ˜‘', 'ðŸ˜µ', 'ðŸ˜¥', 'ðŸ˜ž', 'ðŸ˜¿', 'ðŸ˜¢', 'ðŸ˜•', 'ðŸ˜–', 'ðŸ˜°']\n['ðŸ˜‹', 'ðŸ’›', 'ðŸ˜‰', 'âœŒï¸', 'ðŸ’•', 'ðŸ‘', 'ðŸ˜Ž', 'ðŸ˜œ', 'ðŸ˜', 'ðŸ˜›', 'ðŸ’–', 'ðŸ˜¼', 'ðŸ˜', 'ðŸ˜º', 'ðŸ˜ƒ', 'ðŸ˜¸', 'ðŸ˜„', 'ðŸ™‚', 'ðŸ¤£', 'ðŸ’ž', 'ðŸ˜Œ', 'ðŸ™Œ', 'ðŸ’œ', 'ðŸ™', 'ðŸ‘Œ', 'ðŸ¤“', 'ðŸ¤‘', 'ðŸ‘„', 'ðŸ˜†', 'ðŸ˜™', 'ðŸ˜˜', 'ðŸ˜š', 'ðŸ˜½', 'ðŸ˜—', 'ðŸ’‹', 'ðŸ˜¹', 'ðŸ˜‚', 'ðŸ˜‡', 'ðŸ¤—', 'ðŸ’—', 'ðŸ’“', 'ðŸ˜»', 'ðŸ˜', 'â¤ï¸', 'ðŸ¤', 'ðŸ˜€', 'ðŸ˜', 'ðŸ’š', 'ðŸ’', 'ðŸ¤¤', 'ðŸ’˜', 'ðŸ¤ž', 'ðŸ¤ ', 'ðŸ‘', 'ðŸ˜Š', 'ðŸ’™', 'ðŸ–¤', 'ðŸ’¯', 'ðŸ¤ª']\n54\n59\n"
    }
   ],
   "source": [
    "negativeEmojis = ['ðŸ¤', 'ðŸ˜Ÿ', 'ðŸ˜©', 'ðŸ™ƒ', 'ðŸ˜’', 'ðŸ˜¤', 'ðŸ˜«', 'ðŸ‘Ž', 'ðŸ¤”', 'ðŸ˜…', 'ðŸ˜“', 'ðŸ˜­', 'ðŸ¤§', 'ðŸ˜ˆ', 'ðŸ™','ðŸ˜ª', 'ðŸ˜´', 'â˜ ï¸', 'ðŸ’€', 'ðŸ’©', 'ðŸ™€', 'ðŸ˜±', 'ðŸ™„', 'ðŸ˜¡',\n",
    "'ðŸ‘Š', 'ðŸ˜¾', 'ðŸ˜£', 'ðŸ˜”','ðŸ˜®', 'ðŸ˜¶', 'ðŸ˜', 'ðŸ¤¢', 'ðŸ˜·','ðŸ¤¥', 'ðŸ‘¿','ðŸ˜¯', 'ðŸ˜¬', 'ðŸ‘»', 'ðŸ–•', 'â˜¹ï¸', 'ðŸ˜¦', 'ðŸ˜³', 'ðŸ˜¨', 'ðŸ¤’', 'ðŸ¤•', 'ðŸ˜‘', 'ðŸ˜µ', 'ðŸ˜¥', 'ðŸ˜ž', 'ðŸ˜¿', 'ðŸ˜¢','ðŸ˜•',\n",
    "'ðŸ˜–', 'ðŸ˜°']\n",
    "positiveEmojis = ['ðŸ˜‹', 'ðŸ’›', 'ðŸ˜‰', 'âœŒï¸', 'ðŸ’•', 'ðŸ‘', 'ðŸ˜Ž', 'ðŸ˜œ', 'ðŸ˜', 'ðŸ˜›', 'ðŸ’–', 'ðŸ˜¼', 'ðŸ˜', 'ðŸ˜º', 'ðŸ˜ƒ','ðŸ˜¸', 'ðŸ˜„', 'ðŸ™‚', 'ðŸ¤£', 'ðŸ’ž', 'ðŸ˜Œ', 'ðŸ™Œ', 'ðŸ’œ', 'ðŸ™', 'ðŸ‘Œ', 'ðŸ¤“', 'ðŸ¤‘','ðŸ‘„', 'ðŸ˜†', 'ðŸ˜™', 'ðŸ˜˜', 'ðŸ˜š','ðŸ˜½', 'ðŸ˜—', 'ðŸ’‹', 'ðŸ˜¹', 'ðŸ˜‚', 'ðŸ˜‡', 'ðŸ¤—', 'ðŸ’—', 'ðŸ’“', 'ðŸ˜»', 'ðŸ˜', 'â¤ï¸', 'ðŸ¤', 'ðŸ˜€', 'ðŸ˜', 'ðŸ’š', 'ðŸ’', 'ðŸ¤¤','ðŸ’˜', 'ðŸ¤ž', 'ðŸ¤ ', 'ðŸ‘', 'ðŸ˜Š', 'ðŸ’™', 'ðŸ–¤', 'ðŸ’¯', 'ðŸ¤ª']\n",
    "print(negativeEmojis)\n",
    "print(positiveEmojis)\n",
    "print(len(negativeEmojis))\n",
    "print(len(positiveEmojis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6451\n6451\n6451\n"
    }
   ],
   "source": [
    "# tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "# need to go through and add labels\n",
    "labeledTweetMatrix = []\n",
    "tweets_x = []\n",
    "tweets_y = []\n",
    "tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "# for i in tweetFile:\n",
    "for i in range(10000):\n",
    "    line = tweetFile.readline()\n",
    "    if any(emoji in line for emoji in positiveEmojis):\n",
    "        labeledTweetMatrix.append([line.rstrip(\"\\n\"), 1])\n",
    "        tweets_x.append(line.rstrip(\"\\n\"))\n",
    "        tweets_y.append(1)\n",
    "    elif any(emoji in line for emoji in negativeEmojis):\n",
    "        # labelVector[i] = -1\n",
    "        labeledTweetMatrix.append([line.rstrip(\"\\n\"), -1])\n",
    "        tweets_x.append(line.rstrip(\"\\n\"))\n",
    "        tweets_y.append(-1)\n",
    "\n",
    "    # this will assign all with a +1 emoji 1 and anything else (negative or 0 sentiment) -1\n",
    "print(len(labeledTweetMatrix))\n",
    "print(len(tweets_x))\n",
    "print(len(tweets_y))\n",
    "# print(labeledTweetMatrix[5000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dude is like 5â€™8 140 pounds his dick was long and strong(always the little dudes carrying the ðŸ†) ðŸ¤ªðŸ™ƒ\n1\nI CANT BREATIUHW ðŸ’€ðŸ’€ðŸ’€\n-1\nI am SO scared of birdsðŸ¤§\n-1\nThatâ€™s me ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚\n1\nMy heart is so full rn ðŸ’–ðŸ’–\n1\n"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # print(labeledTweetMatrix[i])\n",
    "    print(tweets_x[i])\n",
    "    print(tweets_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = 'Positive', 'Negative'\n",
    "# go through labeledTweetMatrix and tally positive sentiment, neg just do len(total) - pos\n",
    "count = 0\n",
    "negativeCount = 0\n",
    "for tweet in labeledTweetMatrix:\n",
    "    if tweet[1] == 1:\n",
    "        count += 1\n",
    "    if tweet[1] == -1:\n",
    "        negativeCount += 1\n",
    "sizes = [count, negativeCount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Dana\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "# need to preprocess\n",
    "#pip install tweet-preprocessor needed\n",
    "import nltk                                # Python library for NLP\n",
    "import preprocessor as p\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "def tokenize(tweet):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    return tweet_tokens\n",
    "def removeStopWords(tweet_tokenz):\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    stopwords_english.extend(['rt', 'fav', \"'\"])\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokenz:\n",
    "        if (word not in stopwords_english and word not in string.punctuation):\n",
    "            tweets_clean.append(word)\n",
    "    return tweets_clean\n",
    "def stemmingTweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    tweets_stem = []\n",
    "    for word in tweet:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        tweets_stem.append(stem_word)\n",
    "    return tweets_stem\n",
    "def preprocess(tweet):\n",
    "    tweet.lower()\n",
    "    p.set_options(p.OPT.NUMBER, p.OPT.SMILEY)\n",
    "    tweet = p.clean(tweet)\n",
    "    return stemmingTweet(removeStopWords(tokenize(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dude is like 5â€™8 140 pounds his dick was long and strong(always the little dudes carrying the ðŸ†) ðŸ¤ªðŸ™ƒ\n['dude', 'like', 'â€™', '8', 'pound', 'dick', 'long', 'strong', 'alway', 'littl', 'dude', 'carri', 'ðŸ†', 'ðŸ¤ª', 'ðŸ™ƒ']\n['dude', 'like', 'â€™', '8', 'pound', 'dick', 'long', 'strong', 'alway', 'littl', 'dude', 'carri', 'ðŸ†', 'ðŸ¤ª', 'ðŸ™ƒ']\n['cant', 'breatiuhw', 'ðŸ’€', 'ðŸ’€', 'ðŸ’€']\n['scare', 'bird', 'ðŸ¤§']\n['â€™', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚']\n['heart', 'full', 'rn', 'ðŸ’–', 'ðŸ’–']\n"
    }
   ],
   "source": [
    "# example of preprocessing\n",
    "tweet = tweets_x[0]\n",
    "newtweet = preprocess(tweet)\n",
    "print(tweet)\n",
    "print(newtweet)\n",
    "# preprocess entire matrix\n",
    "for i in range(len(tweets_x)):\n",
    "    processedTweet = preprocess(tweets_x[i])\n",
    "    tweets_x[i] = processedTweet\n",
    "for i in range(5):\n",
    "    print(tweets_x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6451\n4516\n1935\n4516\n1935\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# splitting train/test up 70/30\n",
    "div = round(len(tweets_x)*.7)\n",
    "train_x = tweets_x[:div]\n",
    "test_x = tweets_x[div:]\n",
    "train_y = tweets_y[:div]\n",
    "test_y = tweets_y[div:]\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.array(test_y)\n",
    "print(len(tweets_x))\n",
    "print(len(train_x))\n",
    "print(len(test_x))\n",
    "print(len(train_y))\n",
    "print(len(test_x))\n",
    "assert(len(tweets_x) == len(train_x)+len(test_y))\n",
    "assert(len(tweets_x) == len(train_y)+len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will build a dictionary of word frequencies\n",
    "# train_x and train_y is what we will build our word frequency dictionary from\n",
    "# we will need to make our arrays into lists\n",
    "#import numpy as np\n",
    "def word_freqs(trainx, trainy):\n",
    "    train_x_list = np.squeeze(trainx).tolist()\n",
    "    train_y_list = np.squeeze(trainy).tolist()\n",
    "    frequencyDict = {}\n",
    "    for y, tweetx in zip(train_y_list, train_x_list):\n",
    "        for word in tweetx:\n",
    "            labeledTweet = (word, y)\n",
    "            # if word is in dict, add 1 to count, otherwise add to dict         \n",
    "            if labeledTweet in frequencyDict:\n",
    "                frequencyDict[labeledTweet]+=1\n",
    "            else:\n",
    "                frequencyDict[labeledTweet]=1\n",
    "    return frequencyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{}\n"
    }
   ],
   "source": [
    "# sample\n",
    "freqs = word_freqs(train_x[15000:15005], train_y[15000:15005])\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6762\n"
    }
   ],
   "source": [
    "# build entire frequency dictionary\n",
    "freqDict = word_freqs(train_x, train_y)\n",
    "print(len(freqDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "29225\n8831\n4594\n2168\n"
    }
   ],
   "source": [
    "#get number of unique words\n",
    "uniqueWords = set([pair[0] for pair in freqDict.keys()])\n",
    "\n",
    "numPositive = 0\n",
    "numNegative = 0\n",
    "uniquePositive = 0\n",
    "uniqueNegative = 0\n",
    "\n",
    "for pair in freqDict.keys():\n",
    "    #if this word is positive\n",
    "    if pair[1] > 0:\n",
    "        numPositive += freqDict[pair]\n",
    "        uniquePositive += 1\n",
    "    #if this word is negative\n",
    "    else:\n",
    "        numNegative += freqDict[pair]\n",
    "        uniqueNegative += 1\n",
    "\n",
    "print(numPositive)\n",
    "print(numNegative)\n",
    "print(uniquePositive)\n",
    "print(uniqueNegative)\n",
    "\n",
    "#get total numbers of neg/positive in training data\n",
    "totalTraining = len(train_y)\n",
    "trainPositive = 0\n",
    "trainNegative = 0\n",
    "for item in train_y:\n",
    "    if item > 0:\n",
    "        trainPositive += 1\n",
    "    else:\n",
    "        trainNegative += 1    \n",
    "\n",
    "probPositive = trainPositive/totalTraining\n",
    "probNegative = 1 - probPositive\n",
    "logPrior = np.log(probPositive) - np.log(probNegative)\n",
    "\n",
    "#iterate through all the unique words and figure out log likelihoods\n",
    "#look at lookup function\n",
    "\n",
    "logWords = {}\n",
    "\n",
    "for word in uniqueWords:\n",
    "    pair = (word, 1)\n",
    "    n = 0\n",
    "    if (pair in freqDict):\n",
    "        n = freqDict[pair]\n",
    "    logPos = (n + 1.)/(numPositive + len(uniqueWords))\n",
    "\n",
    "    pair = (word, -1)\n",
    "    n = 0\n",
    "    if (pair in freqDict):\n",
    "        n = freqDict[pair]\n",
    "    logNeg = (n + 1.)/(numNegative + len(uniqueWords))\n",
    "\n",
    "    logWords[word] = np.log(logPos/logNeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['ðŸ˜‹', 21, 0], ['ðŸ’›', 56, 0], ['ðŸ˜‰', 89, 0], ['âœŒï¸', 0, 0], ['ðŸ’•', 212, 0], ['ðŸ‘', 46, 0], ['ðŸ˜Ž', 58, 0], ['ðŸ˜œ', 31, 0], ['ðŸ˜', 24, 0], ['ðŸ˜›', 20, 0], ['ðŸ’–', 127, 0], ['ðŸ˜¼', 4, 0], ['ðŸ˜', 30, 0], ['ðŸ˜º', 0, 0], ['ðŸ˜ƒ', 14, 0], ['ðŸ˜¸', 5, 0], ['ðŸ˜„', 24, 0], ['ðŸ™‚', 15, 0], ['ðŸ¤£', 198, 0], ['ðŸ’ž', 67, 0], ['ðŸ˜Œ', 26, 0], ['ðŸ™Œ', 150, 0], ['ðŸ’œ', 104, 0], ['ðŸ™', 242, 0], ['ðŸ‘Œ', 72, 0], ['ðŸ¤“', 3, 0], ['ðŸ¤‘', 11, 0], ['ðŸ‘„', 6, 0], ['ðŸ˜†', 64, 0], ['ðŸ˜™', 8, 0], ['ðŸ˜˜', 96, 0], ['ðŸ˜š', 14, 0], ['ðŸ˜½', 4, 0], ['ðŸ˜—', 9, 0], ['ðŸ’‹', 47, 0], ['ðŸ˜¹', 5, 0], ['ðŸ˜‚', 1679, 0], ['ðŸ˜‡', 22, 0], ['ðŸ¤—', 86, 0], ['ðŸ’—', 112, 0], ['ðŸ’“', 48, 0], ['ðŸ˜»', 28, 0], ['ðŸ˜', 475, 0], ['â¤ï¸', 0, 0], ['ðŸ¤', 2, 0], ['ðŸ˜€', 32, 0], ['ðŸ˜', 54, 0], ['ðŸ’š', 59, 0], ['ðŸ’', 12, 0], ['ðŸ¤¤', 45, 0], ['ðŸ’˜', 36, 0], ['ðŸ¤ž', 23, 0], ['ðŸ¤ ', 15, 0], ['ðŸ‘', 170, 0], ['ðŸ˜Š', 117, 0], ['ðŸ’™', 90, 0], ['ðŸ–¤', 47, 0], ['ðŸ’¯', 138, 0], ['ðŸ¤ª', 36, 0], ['ðŸ¤', 1, 7], ['ðŸ˜Ÿ', 1, 5], ['ðŸ˜©', 89, 126], ['ðŸ™ƒ', 5, 48], ['ðŸ˜’', 4, 25], ['ðŸ˜¤', 5, 30], ['ðŸ˜«', 12, 18], ['ðŸ‘Ž', 0, 10], ['ðŸ¤”', 12, 95], ['ðŸ˜…', 9, 34], ['ðŸ˜“', 0, 3], ['ðŸ˜­', 292, 426], ['ðŸ¤§', 6, 26], ['ðŸ˜ˆ', 7, 30], ['ðŸ™', 0, 10], ['ðŸ˜ª', 3, 16], ['ðŸ˜´', 3, 20], ['â˜ ï¸', 0, 0], ['ðŸ’€', 52, 71], ['ðŸ’©', 0, 20], ['ðŸ™€', 0, 8], ['ðŸ˜±', 4, 35], ['ðŸ™„', 16, 86], ['ðŸ˜¡', 0, 13], ['ðŸ‘Š', 12, 18], ['ðŸ˜¾', 0, 1], ['ðŸ˜£', 4, 10], ['ðŸ˜”', 8, 39], ['ðŸ˜®', 1, 61], ['ðŸ˜¶', 1, 6], ['ðŸ˜', 1, 16], ['ðŸ¤¢', 3, 7], ['ðŸ˜·', 1, 2], ['ðŸ¤¥', 0, 4], ['ðŸ‘¿', 0, 2], ['ðŸ˜¯', 0, 4], ['ðŸ˜¬', 0, 37], ['ðŸ‘»', 1, 9], ['ðŸ–•', 3, 10], ['â˜¹ï¸', 0, 0], ['ðŸ˜¦', 0, 1], ['ðŸ˜³', 10, 29], ['ðŸ˜¨', 0, 0], ['ðŸ¤’', 0, 6], ['ðŸ¤•', 0, 2], ['ðŸ˜‘', 0, 13], ['ðŸ˜µ', 1, 17], ['ðŸ˜¥', 0, 13], ['ðŸ˜ž', 7, 17], ['ðŸ˜¿', 0, 1], ['ðŸ˜¢', 7, 35], ['ðŸ˜•', 4, 20], ['ðŸ˜–', 0, 1], ['ðŸ˜°', 0, 9]]\n"
    }
   ],
   "source": [
    "# Analyze how many times our emojis appear in our frequency dictionary\n",
    "keys = positiveEmojis + negativeEmojis\n",
    "data = []\n",
    "for emoji in keys:\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    if (emoji, 1) in freqDict:\n",
    "        positive = freqDict[(emoji, 1)]\n",
    "    if (emoji, -1) in freqDict:\n",
    "        negative = freqDict[(emoji, -1)]\n",
    "    data.append([emoji, positive, negative])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output for positive example: 22.081497756192675\nOutput for negative example:  12.032963787976106\n"
    }
   ],
   "source": [
    "def naiveBayes(tweet):\n",
    "    prob = 0\n",
    "    for word in tweet:\n",
    "        if word in logWords:\n",
    "            prob += logWords[word]\n",
    "    prob += logPrior\n",
    "\n",
    "    return prob\n",
    "\n",
    "newTweet = 'I am happy and glad and smiling and I love my life â¤ï¸'\n",
    "prob = naiveBayes(newTweet)\n",
    "print('Output for positive example: ', prob)\n",
    "newTweet1 = 'I am sad and I hate my life and everything sucks ðŸ¤¢'\n",
    "prob = naiveBayes(newTweet1)\n",
    "print('Output for negative example: ', prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.8801033591731267\n"
    }
   ],
   "source": [
    "predictions = []\n",
    "def testAccuracy(test_x, test_y):\n",
    "    for tweet in test_x:\n",
    "        #print(tweet1)\n",
    "        prob = naiveBayes(tweet)\n",
    "        if (prob > 0):\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "    error = np.mean(np.absolute(test_y - predictions))\n",
    "    return 1-error\n",
    "\n",
    "print('Accuracy: ', testAccuracy(test_x, test_y))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}