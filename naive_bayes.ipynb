{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1605377127842",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Squad arriving for Game 2 ğŸš€\n\nDude is like 5â€™8 140 pounds his dick was long and strong(always the little dudes carrying the ğŸ†) ğŸ¤ªğŸ™ƒ\n\nFOLLOWERSğŸ‘‡\n\nI CANT BREATIUHW ğŸ’€ğŸ’€ğŸ’€\n\n2ï¸âƒ£4ï¸âƒ£ hours 'til our schedule drops!\n\n"
    }
   ],
   "source": [
    "tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "number_of_lines = 5\n",
    "for i in range(number_of_lines):\n",
    "    line = tweetFile.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['ğŸ¤', 'ğŸ˜Ÿ', 'ğŸ˜©', 'ğŸ™ƒ', 'ğŸ˜’', 'ğŸ˜¤', 'ğŸ˜«', 'ğŸ‘', 'ğŸ¤”', 'ğŸ˜…', 'ğŸ˜“', 'ğŸ˜­', 'ğŸ¤§', 'ğŸ˜ˆ', 'ğŸ™', 'ğŸ˜ª', 'ğŸ˜´', 'â˜ ï¸', 'ğŸ’€', 'ğŸ’©', 'ğŸ™€', 'ğŸ˜±', 'ğŸ™„', 'ğŸ˜¡', 'ğŸ‘Š', 'ğŸ˜¾', 'ğŸ˜£', 'ğŸ˜”', 'ğŸ˜®', 'ğŸ˜¶', 'ğŸ˜', 'ğŸ¤¢', 'ğŸ˜·', 'ğŸ¤¥', 'ğŸ‘¿', 'ğŸ˜¯', 'ğŸ˜¬', 'ğŸ‘»', 'ğŸ–•', 'â˜¹ï¸', 'ğŸ˜¦', 'ğŸ˜³', 'ğŸ˜¨', 'ğŸ¤’', 'ğŸ¤•', 'ğŸ˜‘', 'ğŸ˜µ', 'ğŸ˜¥', 'ğŸ˜', 'ğŸ˜¿', 'ğŸ˜¢', 'ğŸ˜•', 'ğŸ˜–', 'ğŸ˜°']\n['ğŸ˜‹', 'ğŸ’›', 'ğŸ˜‰', 'âœŒï¸', 'ğŸ’•', 'ğŸ‘', 'ğŸ˜', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜›', 'ğŸ’–', 'ğŸ˜¼', 'ğŸ˜', 'ğŸ˜º', 'ğŸ˜ƒ', 'ğŸ˜¸', 'ğŸ˜„', 'ğŸ™‚', 'ğŸ¤£', 'ğŸ’', 'ğŸ˜Œ', 'ğŸ™Œ', 'ğŸ’œ', 'ğŸ™', 'ğŸ‘Œ', 'ğŸ¤“', 'ğŸ¤‘', 'ğŸ‘„', 'ğŸ˜†', 'ğŸ˜™', 'ğŸ˜˜', 'ğŸ˜š', 'ğŸ˜½', 'ğŸ˜—', 'ğŸ’‹', 'ğŸ˜¹', 'ğŸ˜‚', 'ğŸ˜‡', 'ğŸ¤—', 'ğŸ’—', 'ğŸ’“', 'ğŸ˜»', 'ğŸ˜', 'â¤ï¸', 'ğŸ¤', 'ğŸ˜€', 'ğŸ˜', 'ğŸ’š', 'ğŸ’', 'ğŸ¤¤', 'ğŸ’˜', 'ğŸ¤', 'ğŸ¤ ', 'ğŸ‘', 'ğŸ˜Š', 'ğŸ’™', 'ğŸ–¤', 'ğŸ’¯', 'ğŸ¤ª']\n54\n59\n"
    }
   ],
   "source": [
    "negativeEmojis = ['ğŸ¤', 'ğŸ˜Ÿ', 'ğŸ˜©', 'ğŸ™ƒ', 'ğŸ˜’', 'ğŸ˜¤', 'ğŸ˜«', 'ğŸ‘', 'ğŸ¤”', 'ğŸ˜…', 'ğŸ˜“', 'ğŸ˜­', 'ğŸ¤§', 'ğŸ˜ˆ', 'ğŸ™','ğŸ˜ª', 'ğŸ˜´', 'â˜ ï¸', 'ğŸ’€', 'ğŸ’©', 'ğŸ™€', 'ğŸ˜±', 'ğŸ™„', 'ğŸ˜¡',\n",
    "'ğŸ‘Š', 'ğŸ˜¾', 'ğŸ˜£', 'ğŸ˜”','ğŸ˜®', 'ğŸ˜¶', 'ğŸ˜', 'ğŸ¤¢', 'ğŸ˜·','ğŸ¤¥', 'ğŸ‘¿','ğŸ˜¯', 'ğŸ˜¬', 'ğŸ‘»', 'ğŸ–•', 'â˜¹ï¸', 'ğŸ˜¦', 'ğŸ˜³', 'ğŸ˜¨', 'ğŸ¤’', 'ğŸ¤•', 'ğŸ˜‘', 'ğŸ˜µ', 'ğŸ˜¥', 'ğŸ˜', 'ğŸ˜¿', 'ğŸ˜¢','ğŸ˜•',\n",
    "'ğŸ˜–', 'ğŸ˜°']\n",
    "positiveEmojis = ['ğŸ˜‹', 'ğŸ’›', 'ğŸ˜‰', 'âœŒï¸', 'ğŸ’•', 'ğŸ‘', 'ğŸ˜', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜›', 'ğŸ’–', 'ğŸ˜¼', 'ğŸ˜', 'ğŸ˜º', 'ğŸ˜ƒ','ğŸ˜¸', 'ğŸ˜„', 'ğŸ™‚', 'ğŸ¤£', 'ğŸ’', 'ğŸ˜Œ', 'ğŸ™Œ', 'ğŸ’œ', 'ğŸ™', 'ğŸ‘Œ', 'ğŸ¤“', 'ğŸ¤‘','ğŸ‘„', 'ğŸ˜†', 'ğŸ˜™', 'ğŸ˜˜', 'ğŸ˜š','ğŸ˜½', 'ğŸ˜—', 'ğŸ’‹', 'ğŸ˜¹', 'ğŸ˜‚', 'ğŸ˜‡', 'ğŸ¤—', 'ğŸ’—', 'ğŸ’“', 'ğŸ˜»', 'ğŸ˜', 'â¤ï¸', 'ğŸ¤', 'ğŸ˜€', 'ğŸ˜', 'ğŸ’š', 'ğŸ’', 'ğŸ¤¤','ğŸ’˜', 'ğŸ¤', 'ğŸ¤ ', 'ğŸ‘', 'ğŸ˜Š', 'ğŸ’™', 'ğŸ–¤', 'ğŸ’¯', 'ğŸ¤ª']\n",
    "print(negativeEmojis)\n",
    "print(positiveEmojis)\n",
    "print(len(negativeEmojis))\n",
    "print(len(positiveEmojis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "31330\n31330\n31330\n"
    }
   ],
   "source": [
    "# tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "# need to go through and add labels\n",
    "labeledTweetMatrix = []\n",
    "tweets_x = []\n",
    "tweets_y = []\n",
    "tweetFile = open(\"emojitweets-01-04-2018.txt\", encoding=\"utf8\")\n",
    "# for i in tweetFile:\n",
    "for i in range(50000):\n",
    "    line = tweetFile.readline()\n",
    "    if any(emoji in line for emoji in positiveEmojis):\n",
    "        labeledTweetMatrix.append([line.rstrip(\"\\n\"), 1])\n",
    "        tweets_x.append(line.rstrip(\"\\n\"))\n",
    "        tweets_y.append(1)\n",
    "    elif any(emoji in line for emoji in negativeEmojis):\n",
    "        # labelVector[i] = -1\n",
    "        labeledTweetMatrix.append([line.rstrip(\"\\n\"), -1])\n",
    "        tweets_x.append(line.rstrip(\"\\n\"))\n",
    "        tweets_y.append(-1)\n",
    "\n",
    "    # this will assign all with a +1 emoji 1 and anything else (negative or 0 sentiment) -1\n",
    "print(len(labeledTweetMatrix))\n",
    "print(len(tweets_x))\n",
    "print(len(tweets_y))\n",
    "# print(labeledTweetMatrix[5000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dude is like 5â€™8 140 pounds his dick was long and strong(always the little dudes carrying the ğŸ†) ğŸ¤ªğŸ™ƒ\n1\nI CANT BREATIUHW ğŸ’€ğŸ’€ğŸ’€\n-1\nI am SO scared of birdsğŸ¤§\n-1\nThatâ€™s me ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚\n1\nMy heart is so full rn ğŸ’–ğŸ’–\n1\n"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # print(labeledTweetMatrix[i])\n",
    "    print(tweets_x[i])\n",
    "    print(tweets_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'Positive', 'Negative'\n",
    "# go through labeledTweetMatrix and tally positive sentiment, neg just do len(total) - pos\n",
    "count = 0\n",
    "negativeCount = 0\n",
    "for tweet in labeledTweetMatrix:\n",
    "    if tweet[1] == 1:\n",
    "        count += 1\n",
    "    if tweet[1] == -1:\n",
    "        negativeCount += 1\n",
    "sizes = [count, negativeCount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Dana\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "# need to preprocess\n",
    "#pip install tweet-preprocessor needed\n",
    "import nltk                                # Python library for NLP\n",
    "import preprocessor as p\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "def tokenize(tweet):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    return tweet_tokens\n",
    "def removeStopWords(tweet_tokenz):\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    stopwords_english.extend(['rt', 'fav', \"'\"])\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokenz:\n",
    "        if (word not in stopwords_english and word not in string.punctuation):\n",
    "            tweets_clean.append(word)\n",
    "    return tweets_clean\n",
    "def stemmingTweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    tweets_stem = []\n",
    "    for word in tweet:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        tweets_stem.append(stem_word)\n",
    "    return tweets_stem\n",
    "def preprocess(tweet):\n",
    "    tweet.lower()\n",
    "    p.set_options(p.OPT.NUMBER, p.OPT.SMILEY)\n",
    "    tweet = p.clean(tweet)\n",
    "    return stemmingTweet(removeStopWords(tokenize(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dude is like 5â€™8 140 pounds his dick was long and strong(always the little dudes carrying the ğŸ†) ğŸ¤ªğŸ™ƒ\n['dude', 'like', 'â€™', '8', 'pound', 'dick', 'long', 'strong', 'alway', 'littl', 'dude', 'carri', 'ğŸ†', 'ğŸ¤ª', 'ğŸ™ƒ']\n['dude', 'like', 'â€™', '8', 'pound', 'dick', 'long', 'strong', 'alway', 'littl', 'dude', 'carri', 'ğŸ†', 'ğŸ¤ª', 'ğŸ™ƒ']\n['cant', 'breatiuhw', 'ğŸ’€', 'ğŸ’€', 'ğŸ’€']\n['scare', 'bird', 'ğŸ¤§']\n['â€™', 'ğŸ˜‚', 'ğŸ˜‚', 'ğŸ˜‚']\n['heart', 'full', 'rn', 'ğŸ’–', 'ğŸ’–']\n"
    }
   ],
   "source": [
    "# example of preprocessing\n",
    "tweet = tweets_x[0]\n",
    "newtweet = preprocess(tweet)\n",
    "print(tweet)\n",
    "print(newtweet)\n",
    "# preprocess entire matrix\n",
    "for i in range(len(tweets_x)):\n",
    "    processedTweet = preprocess(tweets_x[i])\n",
    "    tweets_x[i] = processedTweet\n",
    "for i in range(5):\n",
    "    print(tweets_x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "31330\n21931\n9399\n21931\n9399\n"
    }
   ],
   "source": [
    "# splitting train/test up 70/30\n",
    "div = round(len(tweets_x)*.7)\n",
    "train_x = tweets_x[:div]\n",
    "test_x = tweets_x[div:]\n",
    "train_y = tweets_y[:div]\n",
    "test_y = tweets_y[div:]\n",
    "print(len(tweets_x))\n",
    "print(len(train_x))\n",
    "print(len(test_x))\n",
    "print(len(train_y))\n",
    "print(len(test_x))\n",
    "assert(len(tweets_x) == len(train_x)+len(test_y))\n",
    "assert(len(tweets_x) == len(train_y)+len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will build a dictionary of word frequencies\n",
    "# train_x and train_y is what we will build our word frequency dictionary from\n",
    "# we will need to make our arrays into lists\n",
    "import numpy as np\n",
    "def word_freqs(trainx, trainy):\n",
    "    train_x_list = np.squeeze(trainx).tolist()\n",
    "    train_y_list = np.squeeze(trainy).tolist()\n",
    "    frequencyDict = {}\n",
    "    for y, tweetx in zip(train_y_list, train_x_list):\n",
    "        for word in tweetx:\n",
    "            labeledTweet = (word, y)\n",
    "            # if word is in dict, add 1 to count, otherwise add to dict         \n",
    "            if labeledTweet in frequencyDict:\n",
    "                frequencyDict[labeledTweet]+=1\n",
    "            else:\n",
    "                frequencyDict[labeledTweet]=1\n",
    "    return frequencyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{('world', -1): 1, ('map', -1): 1, ('sound', -1): 1, ('like', -1): 1, ('piano', -1): 1, ('ğŸ˜®', -1): 1, ('nigga', 1): 2, ('said', 1): 2, ('insur', 1): 2, ('frog', 1): 2, ('vs', 1): 2, ('ğŸ¸', 1): 6, ('ğŸ˜‚', 1): 7, ('final', 1): 1, ('found', 1): 1, ('video', 1): 1, ('ğŸ˜­', 1): 3, ('yerrr', 1): 1, ('post', 1): 1, ('panti', 1): 1, ('pic', 1): 1, ('get', 1): 1, ('readi', 1): 1, ('guy', 1): 1, ('ğŸ˜', 1): 1}\n"
    }
   ],
   "source": [
    "# sample\n",
    "freqs = word_freqs(train_x[15000:15005], train_y[15000:15005])\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "17426\n"
    }
   ],
   "source": [
    "# build entire frequency dictionary\n",
    "freqDict = word_freqs(train_x, train_y)\n",
    "print(len(freqDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "141406\n42359\n11742\n5684\n105\n1\n1\n1\n5\n"
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "('ğŸ’‰', 1)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-fe2d5d3a7366>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#iterate through all the unique words and figure out log likelihoods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muniqueWords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreqDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: ('ğŸ’‰', 1)"
     ]
    }
   ],
   "source": [
    "#get number of unique words\n",
    "uniqueWords = set([pair[0] for pair in freqDict.keys()])\n",
    "\n",
    "numPositive = 0\n",
    "numNegative = 0\n",
    "uniquePositive = 0\n",
    "uniqueNegative = 0\n",
    "\n",
    "for pair in freqDict.keys():\n",
    "    #if this word is positive\n",
    "    if pair[1] > 0:\n",
    "        numPositive += freqDict[pair]\n",
    "        uniquePositive += 1\n",
    "    #if this word is negative\n",
    "    else:\n",
    "        numNegative += freqDict[pair]\n",
    "        uniqueNegative += 1\n",
    "\n",
    "print(numPositive)\n",
    "print(numNegative)\n",
    "print(uniquePositive)\n",
    "print(uniqueNegative)\n",
    "\n",
    "#get total numbers of neg/positive in training data\n",
    "totalTraining = len(train_y)\n",
    "trainPositive = 0\n",
    "trainNegative = 0\n",
    "for item in train_y:\n",
    "    if item > 0:\n",
    "        trainPositive += 1\n",
    "    else:\n",
    "        trainNegative += 1    \n",
    "\n",
    "probPositive = trainPositive/totalTraining\n",
    "probNegative = 1 - probPositive\n",
    "logPrior = np.log(probPositive) - np.log(probNegative)\n",
    "\n",
    "#iterate through all the unique words and figure out log likelihoods\n",
    "#look at lookup function\n",
    "for word in uniqueWords:\n",
    "    print(freqDict[(word , 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['ğŸ˜‹', 138, 0], ['ğŸ’›', 395, 0], ['ğŸ˜‰', 345, 0], ['âœŒï¸', 0, 0], ['ğŸ’•', 1020, 0], ['ğŸ‘', 336, 0], ['ğŸ˜', 323, 0], ['ğŸ˜œ', 139, 0], ['ğŸ˜', 86, 0], ['ğŸ˜›', 74, 0], ['ğŸ’–', 585, 0], ['ğŸ˜¼', 12, 0], ['ğŸ˜', 185, 0], ['ğŸ˜º', 4, 0], ['ğŸ˜ƒ', 96, 0], ['ğŸ˜¸', 14, 0], ['ğŸ˜„', 133, 0], ['ğŸ™‚', 112, 0], ['ğŸ¤£', 896, 0], ['ğŸ’', 351, 0], ['ğŸ˜Œ', 170, 0], ['ğŸ™Œ', 672, 0], ['ğŸ’œ', 563, 0], ['ğŸ™', 1036, 0], ['ğŸ‘Œ', 403, 0], ['ğŸ¤“', 20, 0], ['ğŸ¤‘', 42, 0], ['ğŸ‘„', 28, 0], ['ğŸ˜†', 174, 0], ['ğŸ˜™', 26, 0], ['ğŸ˜˜', 526, 0], ['ğŸ˜š', 70, 0], ['ğŸ˜½', 11, 0], ['ğŸ˜—', 23, 0], ['ğŸ’‹', 269, 0], ['ğŸ˜¹', 57, 0], ['ğŸ˜‚', 8197, 0], ['ğŸ˜‡', 114, 0], ['ğŸ¤—', 288, 0], ['ğŸ’—', 437, 0], ['ğŸ’“', 328, 0], ['ğŸ˜»', 94, 0], ['ğŸ˜', 2301, 0], ['â¤ï¸', 0, 0], ['ğŸ¤', 14, 0], ['ğŸ˜€', 154, 0], ['ğŸ˜', 327, 0], ['ğŸ’š', 332, 0], ['ğŸ’', 93, 0], ['ğŸ¤¤', 179, 0], ['ğŸ’˜', 227, 0], ['ğŸ¤', 113, 0], ['ğŸ¤ ', 40, 0], ['ğŸ‘', 940, 0], ['ğŸ˜Š', 541, 0], ['ğŸ’™', 511, 0], ['ğŸ–¤', 204, 0], ['ğŸ’¯', 634, 0], ['ğŸ¤ª', 152, 0], ['ğŸ¤', 3, 18], ['ğŸ˜Ÿ', 2, 13], ['ğŸ˜©', 289, 526], ['ğŸ™ƒ', 26, 254], ['ğŸ˜’', 13, 136], ['ğŸ˜¤', 29, 163], ['ğŸ˜«', 44, 153], ['ğŸ‘', 11, 23], ['ğŸ¤”', 68, 486], ['ğŸ˜…', 48, 177], ['ğŸ˜“', 8, 29], ['ğŸ˜­', 1470, 2176], ['ğŸ¤§', 20, 108], ['ğŸ˜ˆ', 55, 179], ['ğŸ™', 3, 37], ['ğŸ˜ª', 11, 80], ['ğŸ˜´', 16, 120], ['â˜ ï¸', 0, 0], ['ğŸ’€', 195, 390], ['ğŸ’©', 8, 50], ['ğŸ™€', 0, 14], ['ğŸ˜±', 51, 171], ['ğŸ™„', 58, 383], ['ğŸ˜¡', 6, 92], ['ğŸ‘Š', 46, 81], ['ğŸ˜¾', 0, 14], ['ğŸ˜£', 14, 64], ['ğŸ˜”', 25, 168], ['ğŸ˜®', 6, 276], ['ğŸ˜¶', 3, 26], ['ğŸ˜', 6, 67], ['ğŸ¤¢', 6, 46], ['ğŸ˜·', 1, 24], ['ğŸ¤¥', 1, 37], ['ğŸ‘¿', 1, 7], ['ğŸ˜¯', 3, 33], ['ğŸ˜¬', 16, 135], ['ğŸ‘»', 8, 24], ['ğŸ–•', 14, 31], ['â˜¹ï¸', 0, 0], ['ğŸ˜¦', 0, 10], ['ğŸ˜³', 42, 189], ['ğŸ˜¨', 0, 32], ['ğŸ¤’', 0, 12], ['ğŸ¤•', 1, 16], ['ğŸ˜‘', 3, 57], ['ğŸ˜µ', 8, 45], ['ğŸ˜¥', 3, 54], ['ğŸ˜', 17, 67], ['ğŸ˜¿', 0, 7], ['ğŸ˜¢', 42, 185], ['ğŸ˜•', 6, 64], ['ğŸ˜–', 3, 16], ['ğŸ˜°', 1, 33]]\n"
    }
   ],
   "source": [
    "# Analyze how many times our emojis appear in our frequency dictionary\n",
    "keys = positiveEmojis + negativeEmojis\n",
    "data = []\n",
    "for emoji in keys:\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    if (emoji, 1) in freqDict:\n",
    "        positive = freqDict[(emoji, 1)]\n",
    "    if (emoji, -1) in freqDict:\n",
    "        negative = freqDict[(emoji, -1)]\n",
    "    data.append([emoji, positive, negative])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}